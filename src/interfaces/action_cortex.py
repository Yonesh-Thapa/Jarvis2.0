#
# File: src/interfaces/action_cortex.py
#
# Description: This module acts as the AI's "motor cortex". It takes the abstract
# plans generated by the PlanningCortex and translates them into executable actions
# through the available interfaces (transducers). This closes the perception-action loop.
#

from __future__ import annotations
from typing import Dict, Any

class ActionCortex:
    """
    Purpose: To execute plans in the environment.
    Mechanism: A dispatcher that calls the appropriate methods on the transducer
    interfaces based on the received plan. In this simulation, actions are printed
    to the console, but in a physical agent, they would control motors or speakers.
    """
    def __init__(self):
        print("Interface Initialized: Action Cortex")

    def execute_plan(self, plan: Dict[str, Any], transducers: Dict[str, Any]) -> None:
        """
        Purpose: To turn a plan into a concrete action.

        Args:
            plan (Dict[str, Any]): The plan from the PlanningCortex.
            transducers (Dict[str, Any]): A dictionary of the available sensory-motor
                                         interfaces (e.g., webcam, microphone).
        """
        action = plan.get("action")
        reason = plan.get('reason', 'No reason provided.')

        print(f"ACTION: Executing {action}. Reason: {reason}")

        if action == "EXPLORE":
            # The abstract plan now connects to a physical action on the transducer.
            webcam = transducers.get("webcam")
            if webcam:
                webcam.jiggle_camera()
        
        elif action == "CONTINUE":
            # No action needed, but we log the state for clarity.
            pass
        
        else:
            print(f"ACTION: Received unknown plan '{action}'. Doing nothing.")